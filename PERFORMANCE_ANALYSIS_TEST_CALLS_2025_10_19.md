# Retell Voice AI - Performance Analysis: Test Call Latency
**Date:** 2025-10-19
**Analyst:** Performance Engineering Team
**Call ID:** `call_f678b963afcae3cea068a43091b`

---

## Executive Summary

**User Report:** "5-6 second delays" during test calls (lange Sprechpausen)
**Analysis Result:** CONFIRMED - Delays range from **2.5-4.1 seconds** per availability check
**Root Cause:** Multi-component latency across LLM decision, backend processing, and response generation
**Business Impact:** Poor user experience, perceived unresponsiveness, potential call abandonment

---

## Test Call Analysis

### Call Metadata
- **Call ID:** `call_f678b963afcae3cea068a43091b`
- **Duration:** 93.85 seconds (1min 34sec)
- **Agent Version:** V115
- **Disconnection:** User hangup (likely due to poor experience)
- **Function Calls:** 7 total (4√ó parse_date, 3√ó check_availability)

### Retell Platform Metrics
```json
{
  "llm": {
    "p50": 2314ms,
    "p90": 2372ms,
    "max": 2386ms
  },
  "e2e": {
    "p50": 3496ms,
    "p90": 3584ms,
    "max": 3606ms
  },
  "tts": {
    "p50": 598ms,
    "max": 601ms
  }
}
```

**Key Finding:** End-to-end latency (p50) is **3.5 seconds** - well above acceptable UX threshold of 2 seconds.

---

## Detailed Latency Breakdown

### Check #1: 13:00 Uhr Availability

| Event | Timestamp | Delta | Component |
|-------|-----------|-------|-----------|
| User confirms "Ja" (date) | 20.456s | - | User input |
| **LLM Decision Delay** | 20.456s ‚Üí 23.137s | **2.681s** | Retell LLM |
| check_availability CALL | 23.137s | - | Function invocation |
| check_availability RESULT | 25.610s | **2.473s** | Backend execution |
| Agent starts response | 26.963s | 1.353s | TTS generation |
| **Total perceived delay** | - | **6.507s** | Full cycle |

**Breakdown:**
- **LLM Decision:** 2.68s (41% of total delay)
- **Backend Execution:** 2.47s (38% of total delay)
- **Response Formulation:** 1.35s (21% of total delay)

---

### Check #2: 14:00 Uhr Availability

| Event | Timestamp | Delta | Component |
|-------|-----------|-------|-----------|
| User confirms "Ja" | 52.266s | - | User input |
| **LLM Decision Delay** | 52.266s ‚Üí 53.928s | **1.662s** | Retell LLM |
| check_availability CALL | 53.928s | - | Function invocation |
| check_availability RESULT | 56.025s | **2.097s** | Backend execution |
| Agent starts response | 57.353s | 1.328s | TTS generation |
| **Total perceived delay** | - | **5.087s** | Full cycle |

**Breakdown:**
- **LLM Decision:** 1.66s (33% of total delay)
- **Backend Execution:** 2.10s (41% of total delay)
- **Response Formulation:** 1.33s (26% of total delay)

---

### Check #3: 11:30 Uhr Availability

| Event | Timestamp | Delta | Component |
|-------|-----------|-------|-----------|
| User says "Ja, bitte" | 82.216s | - | User input |
| **LLM Decision Delay** | 82.216s ‚Üí 84.252s | **2.036s** | Retell LLM |
| check_availability CALL | 84.252s | - | Function invocation |
| check_availability RESULT | 86.893s | **2.641s** | Backend execution |
| Agent starts response | 88.189s | 1.296s | TTS generation |
| **Total perceived delay** | - | **5.973s** | Full cycle |

**Breakdown:**
- **LLM Decision:** 2.04s (34% of total delay)
- **Backend Execution:** 2.64s (44% of total delay)
- **Response Formulation:** 1.30s (22% of total delay)

---

## Performance Summary

### Aggregate Latency Statistics

| Metric | Check #1 | Check #2 | Check #3 | Average | Target |
|--------|----------|----------|----------|---------|--------|
| **LLM Decision** | 2.68s | 1.66s | 2.04s | **2.13s** | <1.5s |
| **Backend Execution** | 2.47s | 2.10s | 2.64s | **2.40s** | <1.0s |
| **Response Formulation** | 1.35s | 1.33s | 1.30s | **1.33s** | <1.0s |
| **Total Perceived Delay** | 6.51s | 5.09s | 5.97s | **5.86s** | <3.0s |

**Verdict:** All checks exceed target by **95% on average** (5.86s vs 3.0s target)

---

## Bottleneck Analysis

### 1. LLM Decision Latency (2.13s avg)

**What it is:** Time between user utterance and function call invocation
**Why it matters:** This is pure "thinking time" - the agent deciding what to do next

**Current Performance:**
- Average: 2.13s
- Range: 1.66s - 2.68s
- Target: <1.5s

**Root Causes:**
- **Gemini 2.5 Flash model latency:** Platform-level, limited control
- **Prompt complexity:** V115 prompt includes extensive context (customer data, appointment rules, conversational history)
- **Token count:** Average 1,224 tokens per request (Retell metrics)

**Is this acceptable?**
- ‚ö†Ô∏è **PARTIALLY** - While platform-constrained, 2.1s average is on the high end
- Retell's own metrics show p50 of 2.3s, p90 of 2.4s ‚Üí we're consistent with platform behavior
- However, p50 >2s creates noticeable pauses in conversation flow

**Optimization Potential:**
- ‚úÖ **Prompt optimization:** Reduce context window where possible
- ‚úÖ **Streaming responses:** Enable partial response streaming (if Retell supports)
- ‚ùå **Model change:** Limited - Gemini 2.5 Flash already fastest option

---

### 2. Backend Execution Latency (2.40s avg)

**What it is:** Time to execute `check_availability` function (Laravel ‚Üí Cal.com ‚Üí response)
**Why it matters:** This is the ONLY component fully under our control

**Current Performance:**
- Average: 2.40s
- Range: 2.10s - 2.64s
- Target: <1.0s

**Component Breakdown (estimated from logs):**

```
Backend Execution (2.40s total)
‚îú‚îÄ Laravel request overhead: ~50ms
‚îú‚îÄ Call context lookup: ~100ms
‚îú‚îÄ Service selection: ~50ms
‚îú‚îÄ Cal.com API call: ~1,800ms (75% of backend time!)
‚îÇ  ‚îú‚îÄ HTTP request: ~50ms
‚îÇ  ‚îú‚îÄ Cal.com processing: ~1,500ms
‚îÇ  ‚îî‚îÄ HTTP response: ~250ms
‚îú‚îÄ Alternative finder logic: ~300ms
‚îî‚îÄ Response formatting: ~100ms
```

**Critical Finding:** Cal.com API accounts for **75% of backend execution time**

**Cal.com API Performance:**
- Current timeout: 3s (line 224 in CalcomService.php)
- Actual performance: 1.5-2.0s per request
- Cache hit rate: Unknown (need monitoring)
- Cache TTL: 60s (optimized from 300s on 2025-10-11)

**Is this acceptable?**
- ‚ùå **NO** - 2.4s backend execution is **140% above target**
- Cal.com API latency is the PRIMARY bottleneck
- Alternative finder adds unnecessary 300ms overhead

**Optimization Potential:**
- ‚úÖ **HIGH** - Cal.com cache optimization (see recommendations)
- ‚úÖ **MEDIUM** - Parallel processing (alternatives + Cal.com)
- ‚úÖ **LOW** - Database query optimization (already optimized in Phase 4)

---

### 3. Response Formulation Latency (1.33s avg)

**What it is:** Time between function result and agent speaking
**Why it matters:** TTS generation + LLM response formulation

**Current Performance:**
- Average: 1.33s
- Range: 1.30s - 1.35s
- Target: <1.0s

**Component Breakdown:**
- **TTS generation:** ~600ms (Retell metrics: p50 = 598ms)
- **LLM response formulation:** ~730ms (remaining time)

**Is this acceptable?**
- ‚ö†Ô∏è **MARGINAL** - 1.33s is 33% above target
- TTS latency (600ms) is platform-constrained
- Response formulation (730ms) indicates complex prompt processing

**Optimization Potential:**
- ‚ùå **LOW** - TTS latency is platform-level
- ‚úÖ **MEDIUM** - Simplify response templates in prompt
- ‚úÖ **LOW** - Pre-cache common response patterns

---

## Comparison to Performance Targets

### Target vs. Actual Performance

| Metric | Target | Current | Gap | Status |
|--------|--------|---------|-----|--------|
| **LLM Decision** | <1.5s | 2.13s | +42% | üî¥ CRITICAL |
| **Backend Execution** | <1.0s | 2.40s | +140% | üî¥ CRITICAL |
| **Response Formulation** | <1.0s | 1.33s | +33% | üü° WARNING |
| **Total E2E Latency** | <3.0s | 5.86s | +95% | üî¥ CRITICAL |

### User Experience Impact

| Latency Range | User Perception | Status |
|---------------|----------------|--------|
| 0-1s | Instant, natural | ‚úÖ Ideal |
| 1-2s | Acceptable, slight pause | üü¢ Good |
| 2-3s | Noticeable delay | üü° Marginal |
| 3-4s | Awkward silence | üî¥ Poor |
| **4-6s** | **Unacceptable, user frustration** | **üî¥ FAILING** |

**Current performance (5.86s avg)** falls into the **UNACCEPTABLE** range.

---

## Root Cause Analysis

### Why are we experiencing 5-6 second delays?

#### Immediate Causes:
1. **Cal.com API Latency (1.5-2.0s)** ‚Üí PRIMARY BOTTLENECK
2. **LLM Processing Time (2.1s)** ‚Üí Platform limitation
3. **Sequential Processing** ‚Üí No parallelization
4. **Alternative Finder Overhead (300ms)** ‚Üí Unnecessary for simple checks

#### Systemic Causes:
1. **No aggressive caching strategy** ‚Üí Every request hits Cal.com API
2. **Lack of request-level caching** ‚Üí Multiple calls in same conversation re-fetch same data
3. **No predictive prefetching** ‚Üí Could pre-fetch common time slots
4. **No circuit breaker fast-fail** ‚Üí Waits full timeout on Cal.com errors

#### Contributing Factors:
1. **Complex prompt (1,224 tokens avg)** ‚Üí Increases LLM processing time
2. **No streaming responses** ‚Üí User waits for complete generation
3. **Verbose response templates** ‚Üí Longer TTS generation

---

## V88 Prompt Analysis: "IMMEDIATELY call"

### Prompt Directive:
```
"IMMEDIATELY call check_availability function"
```

### Is it working?

**YES** - LLM is calling the function promptly after user confirmation:
- Check #1: 2.68s decision time
- Check #2: 1.66s decision time
- Check #3: 2.04s decision time

**However**, "IMMEDIATELY" is misleading:
- The directive only affects **when** the LLM decides to call the function
- It does NOT reduce **platform-level LLM latency**
- LLM still needs 1.5-2.5s to process context and generate the call

**Verdict:** ‚úÖ Directive is working as intended, but does not address root latency issues

---

## Optimization Recommendations

### HIGH PRIORITY (>1s potential reduction)

#### 1. Aggressive Cal.com Caching Strategy
**Impact:** üî• **-1.5s to -2.0s** (eliminate Cal.com API latency on cache hits)

**Current State:**
- Cache TTL: 60s
- Cache hit rate: Unknown
- Cache invalidation: Event-driven (on booking)

**Recommendations:**
```php
// Implement 3-tier caching strategy:

// TIER 1: Request-level cache (in-memory, PHP session)
// - Cache for duration of single call
// - Prevents duplicate API calls in same conversation
// - TTL: 5 minutes (call duration)

// TIER 2: Redis cache (current implementation)
// - Current 60s TTL is good
// - Add cache warming on call start
// - TTL: 60s (optimized)

// TIER 3: Predictive prefetching
// - Pre-fetch common time slots (9-17h, next 7 days) on call start
// - Background job refreshes every 2 minutes
// - TTL: 2 minutes
```

**Implementation:**
```php
// RetellFunctionCallHandler.php:checkAvailability()

// Check request-level cache first
$cacheKey = "call:{$callId}:slot:{$date}:{$time}";
if ($cachedResult = $this->callLifecycle->getCache($cacheKey)) {
    Log::info('‚ö° Request-level cache HIT', ['latency_saved_ms' => 1800]);
    return $cachedResult;
}

// Fall through to existing Cal.com cache logic
$result = $this->calcomService->getAvailableSlots(...);
$this->callLifecycle->setCache($cacheKey, $result, ttl: 300); // 5min
```

**Expected Result:**
- Cache hit rate: 60-80%
- Latency reduction: 1.5-2.0s per cached request
- Backend execution: 2.4s ‚Üí 0.4s (on cache hit)

---

#### 2. Parallel Alternative Processing
**Impact:** üî• **-300ms to -500ms** (eliminate sequential alternative finding)

**Current State:**
```php
// Sequential processing:
1. Cal.com API call (1.8s)
2. Alternative finder (300ms)
3. Response formatting (100ms)
Total: 2.2s
```

**Recommended State:**
```php
// Parallel processing:
Promise::all([
    'availability' => $calcomService->getAvailableSlots(...),
    'alternatives' => $alternativeFinder->getAlternatives(...)
]);
Total: max(1.8s, 300ms) = 1.8s
Savings: 300ms
```

**Implementation:**
```php
use Illuminate\Support\Facades\Parallel;

$results = Parallel::run([
    fn() => $this->calcomService->getAvailableSlots(...),
    fn() => $this->alternativeFinder->getAlternatives(...)
]);

[$availability, $alternatives] = $results;
```

**Expected Result:**
- Backend execution: 2.4s ‚Üí 2.1s
- Reduction: 300ms

---

### MEDIUM PRIORITY (200-500ms potential reduction)

#### 3. Cal.com Timeout Optimization
**Impact:** üü° **-200ms to -500ms** (faster failure, prevent worst-case hangs)

**Current State:**
- Timeout: 3s (line 224)
- No retry logic (removed in Phase 4)
- Circuit breaker enabled

**Recommendations:**
```php
// Reduce timeout to 2s for interactive calls
// Cal.com API should respond <1.5s; 2s allows headroom
->timeout(2)  // Changed from 3s

// Add timeout logging for monitoring
if ($duration > 2000) {
    Log::warning('Cal.com timeout exceeded', [
        'duration_ms' => $duration,
        'call_id' => $callId
    ]);
}
```

**Expected Result:**
- Average latency: unchanged (Cal.com usually responds <1.5s)
- Worst-case latency: 3s ‚Üí 2s
- Faster failure ‚Üí better UX than hanging

---

#### 4. Prompt Token Reduction
**Impact:** üü° **-200ms to -400ms** (reduce LLM processing time)

**Current State:**
- Average tokens: 1,224 per request
- Complex context window includes full appointment history

**Recommendations:**
```
1. Reduce conversational history window (10 turns ‚Üí 5 turns)
2. Remove redundant system instructions from each request
3. Simplify customer data context (only include what's needed for current operation)
4. Use token-efficient response templates
```

**Expected Result:**
- Token count: 1,224 ‚Üí 800-900 tokens
- LLM latency: 2.13s ‚Üí 1.7-1.9s
- Reduction: 200-400ms

---

#### 5. Streaming Response Implementation
**Impact:** üü° **Perceived latency -1.0s to -1.5s** (user hears response start sooner)

**Current State:**
- TTS generated fully before playback starts
- User waits for complete response

**Recommendations:**
```
Enable Retell streaming mode (if available):
- Agent starts speaking first word as soon as generated
- Remaining response streams in background
- User perceives <1s latency instead of 1.3s
```

**Expected Result:**
- Actual latency: unchanged
- **Perceived latency: -1.0s to -1.5s** (psychological improvement)
- UX: significant improvement

---

### LOW PRIORITY (<200ms potential reduction)

#### 6. Database Query Optimization
**Impact:** üü¢ **-50ms to -100ms** (already optimized in Phase 4)

**Current State:**
- Eager loading implemented
- N+1 queries eliminated
- Indexed lookups

**Recommendations:**
- Monitor slow queries (>100ms)
- Add composite indexes if needed
- Consider read replicas for high load

**Expected Result:**
- Marginal improvement (<100ms)

---

## Performance Targets: Revised Roadmap

### Phase 1: Quick Wins (Week 1)
**Target:** Reduce average latency from 5.86s to **3.5s** (-40%)

Implement:
1. Request-level caching (CallLifecycleService)
2. Cal.com timeout reduction (3s ‚Üí 2s)
3. Monitoring dashboard

**Expected Result:**
- Cache hit rate: 60%
- Backend latency: 2.4s ‚Üí 1.0s (on cache hit)
- Total E2E: 5.86s ‚Üí 3.5s

---

### Phase 2: Parallel Processing (Week 2)
**Target:** Reduce average latency from 3.5s to **3.0s** (-15%)

Implement:
1. Parallel alternative processing
2. Predictive prefetching

**Expected Result:**
- Backend latency: 1.0s ‚Üí 0.7s
- Total E2E: 3.5s ‚Üí 3.0s

---

### Phase 3: Prompt Optimization (Week 3)
**Target:** Reduce average latency from 3.0s to **2.5s** (-17%)

Implement:
1. Token reduction (1,224 ‚Üí 900 tokens)
2. Response template simplification
3. Streaming responses (if available)

**Expected Result:**
- LLM latency: 2.13s ‚Üí 1.7s
- Response formulation: 1.33s ‚Üí perceived 0.5s
- Total E2E: 3.0s ‚Üí 2.5s (perceived)

---

### Phase 4: Advanced Optimization (Month 2)
**Target:** Reduce average latency from 2.5s to **<2.0s** (-20%)

Implement:
1. Edge caching (CDN for Cal.com responses)
2. Database read replicas
3. Horizontal scaling

**Expected Result:**
- Total E2E: 2.5s ‚Üí <2.0s
- **TARGET ACHIEVED** ‚úÖ

---

## Monitoring Recommendations

### Key Performance Indicators (KPIs)

1. **End-to-End Latency (E2E)**
   - Target: <3.0s
   - Alert: >4.0s
   - Critical: >5.0s

2. **Backend Execution Time**
   - Target: <1.0s
   - Alert: >1.5s
   - Critical: >2.0s

3. **Cal.com API Latency**
   - Target: <800ms
   - Alert: >1.5s
   - Critical: >2.0s

4. **Cache Hit Rate**
   - Target: >70%
   - Warning: <50%
   - Critical: <30%

### Monitoring Implementation

```php
// Add to RetellFunctionCallHandler::checkAvailability()

use App\Services\Monitoring\PerformanceMonitor;

$monitor = app(PerformanceMonitor::class);
$monitor->startOperation('check_availability');

try {
    // ... existing code ...

    $monitor->recordMetric('calcom_api_latency', $calcomDuration);
    $monitor->recordMetric('backend_execution', $backendDuration);
    $monitor->recordMetric('cache_hit', $cacheHit ? 1 : 0);

} finally {
    $monitor->endOperation();
}
```

### Dashboards

1. **Real-time Performance Dashboard**
   - Average E2E latency (last 100 calls)
   - Cal.com API latency distribution
   - Cache hit rate
   - Error rate

2. **Historical Trends**
   - Daily average latency
   - P50, P90, P99 percentiles
   - Cache performance over time
   - User abandonment rate

---

## Conclusion

### Current State Assessment

**Status:** üî¥ **CRITICAL PERFORMANCE ISSUE**

- Average latency: **5.86 seconds** (95% above target)
- User perception: **Unacceptable** (awkward silences, frustration)
- Root cause: **Cal.com API latency (75% of backend time)**

### Immediate Actions Required

1. ‚úÖ **Implement request-level caching** (Week 1)
2. ‚úÖ **Reduce Cal.com timeout to 2s** (Week 1)
3. ‚úÖ **Deploy monitoring dashboard** (Week 1)
4. ‚úÖ **Parallel alternative processing** (Week 2)

### Expected Outcome

With Phase 1-2 optimizations:
- Current: **5.86s average**
- Target: **3.0s average**
- Improvement: **-49% latency reduction**
- User experience: **Marginal ‚Üí Good**

### Long-term Goal

- **Target: <2.0s end-to-end latency**
- **Achievable:** YES (with Phase 3-4 optimizations)
- **Timeline:** 2 months

---

## Technical References

### Files Analyzed
- `/var/www/api-gateway/storage/logs/laravel.log` (call transcript)
- `/var/www/api-gateway/app/Services/CalcomService.php` (timeout config)
- `/var/www/api-gateway/app/Http/Controllers/RetellFunctionCallHandler.php` (function handler)

### Call Data
- Call ID: `call_f678b963afcae3cea068a43091b`
- Duration: 93.85s
- Function calls: 7 (4√ó parse_date, 3√ó check_availability)
- Agent version: V115

### Platform Metrics (Retell)
- LLM: Gemini 2.5 Flash
- TTS: OpenAI TTS
- Average tokens: 1,224 per request

---

**Report generated:** 2025-10-19
**Next review:** After Phase 1 implementation (Week 1)
